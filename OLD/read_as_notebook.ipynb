{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# General usage\n","import math\n","import numpy as np\n","import pandas as pd\n","\n","# Reporting\n","from pandas_profiling import ProfileReport\n","\n","# Preprocessing\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer, SimpleImputer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","\n","# Modeling\n","from sklearn import datasets, linear_model\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, matthews_corrcoef, confusion_matrix, precision_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score\n","\n","# Plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n","\n","## Loading data ----\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv('raw/modeling_set.csv')\n","full_execution = False\n","\n","target = 'round_winner'\n","features = [column for column in df.columns if column != target]\n","\n","X, X_val, y, y_val = train_test_split(\n","\tdf[features],\n","\tdf[target],\n","\ttest_size=0.3,\n","\trandom_state=1,\n","\tstratify=df[target])\n","\n","print(X.shape)\n","print(X_val.shape)\n","print(y.shape)\n","print(y_val.shape)\n","\n","# To have all the columns in the same DataFrame\n","df = X\n","df[target] = y\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target = 'round_winner'\n","features = [column for column in df.columns if column != target]\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.set_option('display.max_rows', 100)\n","df.describe().T\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if full_execution:\n","    profile = ProfileReport(df, title=\"CS:GO >> Before\", minimal=True)\n","    profile.to_file(\"storage/df_report_before.html\")\n","\n","### Remove and filtering ----\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Remove fully NA columns\n","print(f'Shape before {df.shape}')\n","df.dropna(axis='columns', how='all', inplace=True)\n","print(f'Shape after {df.shape}')\n","\n","# Remove constant value columns\n","print(f'Shape before {df.shape}')\n","col_unique = df.columns[df.nunique()==1]\n","df.drop(col_unique, axis=1, inplace=True)\n","print(f'Shape after {df.shape}')\n","\n","# Remove high cardinality columns\n","cardinality_list = df.apply(pd.Series.nunique)/df.shape[0]*100\n","cardinality_list.round(1).sort_values(ascending=False)\n","\n","# Remove columns with high ratio of missing values\n","print(f'Shape before {df.shape}')\n","na_threshold = len(df) * .90\n","df = df.dropna(thresh=na_threshold, axis=1)\n","print(f'Shape after {df.shape}')\n","\n","# Remove duplicate rows\n","print(f'Number of duplicates {df.duplicated().sum()} rows to be removed')\n","df.drop_duplicates(inplace=True)\n","\n","#### Correlation ----\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.cluster import hierarchy\n","from sklearn.preprocessing import StandardScaler\n","\n","def correlation_plot(df):\n","    scaler = StandardScaler()\n","\n","    df_ = df.select_dtypes(exclude=['object'])\n","    df_['bomb_planted'] = 1.*df_['bomb_planted']\n","    df_ = scaler.fit_transform(df_)\n","    cov = np.cov(df_, rowvar=False)\n","    order = np.array(hierarchy.dendrogram(hierarchy.ward(cov),no_plot=True)['ivl'], dtype=\"int\")\n","\n","    plt.imshow(cov[order, :][:, order])\n","\n","# https://stackoverflow.com/a/63536382/3780957\n","def correlation_filer(x: pd.DataFrame, bound: float):\n","    \"\"\"Filters the provided DataFrame based on the threshold defined at 'bound' parameter.\n","\n","    Args:\n","        x (pd.DataFrame): Source DataFrame\n","        bound (float): Threshold to limit the correlation.\n","\n","    Returns:\n","        [type]: Filtered DataFrame\n","    \"\"\"\n","    xCorr = x.corr()\n","    xFiltered = xCorr[((xCorr >= bound) | (xCorr <= -bound)) & (xCorr !=1.000)]\n","    xFlattened = xFiltered.unstack().sort_values().drop_duplicates()\n","    return xFlattened\n","\n","correlation_plot(df)\n","cor_ = correlation_filer(df, .8)\n","\n","# https://stackoverflow.com/a/25733562/3780957\n","\n","print(f'Shape before {df.shape}')\n","df.drop(cor_.reset_index()['level_1'], axis=1, inplace=True)\n","print(f'Shape after {df.shape}')\n","\n","\n","\n","#### Column names ----\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["column_target = \"round_winner\"\n","column_target_encoded = \"round_winner_encoded\"\n","column_map_encoded = 'map_encoded'\n","\n","column_features = list(set(df.columns) - set([column_target]))\n","column_features.remove('map')\n","column_features.append(column_map_encoded)\n","\n","# columns_cat = df.select_dtypes(include=['object']).columns\n","# columns_cat = list(set(columns_cat) - set([column_target]))\n","\n","columns_float = ['bomb_planted', 'ct_health', 't_health', 'ct_armor', 't_armor', 'ct_money', 't_money']\n","\n","## Feature selector ----\n","# C:\\Users\\R100983\\OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 3\\Notebooks on feature engineering\\feature importance.ipynb\n","\n","#### TargetEncoder ----\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# OneDrive/GMBD/MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)/Session 4 - Feature Engineering/FE BlindCredit example (original 2).ipynb\n","\n","from sklearn.preprocessing import LabelEncoder\n","import category_encoders as ce\n","\n","le = LabelEncoder()\n","df[column_map_encoded] = le.fit_transform(df['map'])\n","df[column_target_encoded] = le.fit_transform(df[column_target])\n","\n","encoder = ce.target_encoder.TargetEncoder(cols=column_map_encoded)\n","X_transformed = encoder.fit_transform(df[column_features], df[column_target_encoded])\n","\n","### TargetRobustScalerEncoder ----\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# C:\\Users\\juanb\\OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 9 - Forum - Dimensionality Reduction\\Notebook on PCA\\PCA solved_v2.ipynb\n","# Set a variable (features) with the names of all the features BUT the target variable.\n","from sklearn.preprocessing import RobustScaler\n","\n","cols_ = X_transformed.columns\n","scaler = RobustScaler().fit(X_transformed)\n","X_transformed = scaler.transform(X_transformed)\n","X_transformed = pd.DataFrame(X_transformed, columns = cols_)\n","y_transformed = df[column_target_encoded]\n","\n","#### SymbolicTransformer ----\n","\n","# C:\\Users\\juanb\\OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 4 - Feature Engineering\\FE BlindCredit example (original 2).ipynb\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# C:\\Users\\juanb\\OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 4 - Feature Engineering\\FE BlindCredit example (original 2).ipynb\n","\n","from gplearn.genetic import SymbolicTransformer\n","\n","def symbolic_transformer(X_transformed, y_transformed):\n","    function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log',\n","                    'abs', 'neg', 'inv', 'max', 'min']\n","    gp = SymbolicTransformer(generations=10,\n","                            population_size=1000,\n","                            hall_of_fame=100,\n","                            n_components=12,\n","                            function_set=function_set,\n","                            parsimony_coefficient=0.0005,\n","                            max_samples=0.9,\n","                            verbose=1,\n","                            random_state=123,\n","                            n_jobs=-1)\n","    gp.fit(X_transformed, y_transformed)\n","    gp_features = gp.transform(X_transformed)\n","\n","    return gp_features\n","\n","gp_features = symbolic_transformer(X_transformed, y_transformed)\n","\n","print(f'{gp_features.shape[1]} new features generated')\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# C:\\Users\\juanb\\OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 4 - Feature Engineering\\FE BlindCredit example (original 2).ipynb\n","# https://stackoverflow.com/a/32801939/3780957\n","\n","# Build a dataframe from the set of best new features generated.\n","new_dataframe = pd.DataFrame(gp_features)\n","# Set the name of the dataframe columns as 'gp#' where # is a number.\n","new_dataframe.columns = ['gp{}'.format(i) for i in range(len(list(new_dataframe)))]\n","\n","# Add the new dataframe as new columns of my dataset (update the variable X)\n","X_transformed = pd.concat([X_transformed, new_dataframe], axis=1, sort=False)\n","\n","### ReliefF ----\n","# Feature importance\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# C:\\Users\\R100983\\OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 3\\Notebooks on feature engineering\\feature importance.ipynb\n","\n","from typing import List\n","from skrebate import ReliefF\n","import random\n","\n","def plot_importance(features: List[str], importances: List[float]):\n","\n","    num_features = len(features)\n","    indices = np.argsort(importances)\n","\n","    plt.figure(figsize=(8, 10))\n","    plt.title(\"Feature importances\")\n","    plt.barh(range(num_features), importances[indices],\n","             color=\"r\",\n","             xerr=np.std(importances),\n","             align=\"center\")\n","    plt.yticks(range(num_features), features[indices])\n","    plt.ylim([-1, num_features])\n","    plt.show()\n","\n","def importance_relieff(X, y, n_features_to_select=30, n_neighbors=20, sample_rows=1000, plot = True):\n","\n","    sample = random.sample(list(X_transformed.index), sample_rows)\n","    sample_features = X.iloc[sample, :].to_numpy()\n","    sample_labels = y.iloc[sample].to_numpy()\n","\n","    fs = ReliefF(n_features_to_select=n_features_to_select, n_neighbors=n_neighbors)\n","    fs.fit(sample_features, sample_labels)\n","    my_important_features = fs.transform(sample_features)\n","\n","    print(\"No. of tuples, No. of Columns before ReliefF : \"+str(sample_features.shape)+\n","        \"\\nNo. of tuples , No. of Columns after ReliefF : \"+str(my_important_features.shape))\n","\n","    # Plot the importances, taken from the `fs` variable.\n","    if plot:\n","        plot_importance(X.columns, abs(fs.feature_importances_))\n","\n","    # Get the most important column names\n","    my_important_features_names = [X.columns[i] for i in abs(fs.top_features_)]\n","\n","    # Create a DataFrame\n","    X_important = pd.DataFrame(X, columns=my_important_features_names[:my_important_features.shape[1]])\n","\n","    return X_important\n","\n","X_important = importance_relieff(X=X_transformed, y=df[column_target_encoded], n_features_to_select=30, n_neighbors=20, sample_rows=1000)\n","\n","# TODO: permutation_importance\n","\n","### Outliers ----\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import IsolationForest\n","\n","X, y = X_important, y_transformed\n","print(f'Shape before {X_important.shape}')\n","\n","iso = IsolationForest(contamination=0.1)\n","y_pred = iso.fit_predict(X)\n","\n","mask = y_pred != -1\n","X, y = X.loc[list(mask), :], y[list(mask)]\n","\n","print(f'Shape after {X.shape}')\n","\n","### PCA ----\n","# TODO: Usar PCA\n","# C:\\Users\\juanb\\OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 9 - Forum - Dimensionality Reduction\\Notebook on PCA\\PCA_v2.ipynb\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.decomposition import PCA\n","from matplotlib.cm import get_cmap\n","\n","name = \"Accent\"\n","cmap = get_cmap(name)  # type: matplotlib.colors.ListedColormap\n","colors = cmap.colors  # type: list\n","\n","def pca_transform(data, target, n=2):\n","    pca = PCA(n_components=n)\n","    principalComponents = pca.fit_transform(data.drop([target], axis=1))\n","    explained_pca = pca.explained_variance_ratio_\n","\n","    data_pca1 = (\n","        pd.DataFrame(data=principalComponents).reset_index().drop(columns=\"index\")\n","    )\n","    data_pca2 = data.loc[:, target].reset_index().drop(columns=\"index\")\n","    data_pca = pd.concat([data_pca1, data_pca2], axis=1, ignore_index=True)\n","\n","    # Setting columns name\n","    columns = [f\"PC{s}\" for s in range(1, n + 1)]\n","    columns.append(target)\n","    data_pca.columns = columns\n","\n","    return data_pca, explained_pca\n","\n","# https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n","def pca_plot_scatter(data, target, axis1=1, axis2=2):\n","    fig = plt.figure(figsize=(8, 8))\n","    ax = fig.add_subplot(1, 1, 1)\n","    ax.set_xlabel(f\"Principal Component {axis1}\", fontsize=15)\n","    ax.set_ylabel(f\"Principal Component {axis2}\", fontsize=15)\n","    ax.set_title(\"Component PCA\", fontsize=20)\n","    targets = data[target].unique()\n","    for target_, color in zip(targets, colors):\n","        indicesToKeep = data[target] == target_\n","        ax.scatter(\n","            data.loc[indicesToKeep, f\"PC{axis1}\"],\n","            data.loc[indicesToKeep, f\"PC{axis2}\"],\n","            color=color,\n","            s=50,\n","        )\n","    ax.legend(targets)\n","    ax.grid()\n","\n","def pca_plot_density(data, target):\n","    categories = data[target].unique()\n","    category_series = data[target]\n","\n","    fig, axs = plt.subplots(3, 3)\n","    fig.set_size_inches(14, 10)\n","\n","    feature_names = list(set(data.columns) - set([target]))\n","\n","    for subplot, feature in enumerate(feature_names):\n","        x, y = int(subplot / 3), subplot % 3\n","        for value in data[target].unique():\n","            sns.distplot(\n","                data[feature][category_series == value],\n","                hist=False,\n","                kde=True,\n","                kde_kws={\"shade\": True},\n","                label=str(value),\n","                ax=axs[x, y],\n","            )\n","        axs[x, y].set_title(feature)\n","\n","    plt.tight_layout()\n","    plt.show()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.concat([X.reset_index().drop(columns=\"index\"), y.reset_index().drop(columns=\"index\")], axis=1)\n","df_pca, df_explained_variance = pca_transform(data=data, target=column_target_encoded, n=7)\n","print(df_explained_variance.round(2))\n","pca_plot_scatter(data=df_pca, target=column_target_encoded, axis1=1, axis2=2)\n","\n","# TODO: Pensar en poner QDA?\n","# TODO: Tree Pruning\n","# TODO: XGboost\n","\n","\n","## Model ----"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# C:\\Users\\juanb\\OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 5 - EvaluationMetrics\\Evaluation and Validation.ipynb\n","\n","from sklearn.model_selection import cross_val_score, StratifiedKFold\n","\n","def plot_scores(scores, labels):\n","    \"\"\"\n","    Receives scores (one or several arrays) and plots a scatter to the left with\n","    the values of the first one, and a boxplot with all of them to the right.\n","    \n","    Arguments\n","        scores: single list of scores, or list of lists of scores.\n","        labels: single label or list of labels identifying the scores passed\n","    \"\"\"\n","    plt.figure(figsize=(10, 5))\n","    \n","    plt.subplot(1, 2, 1)\n","    plt.title('Scores from {}.'.format(labels[0]))\n","    plt.scatter(range(len(scores[0])), scores[0])\n","    \n","    plt.subplot(1, 2, 2)\n","    plt.title('{} scores stdev={:.4f}'.format(labels[0], np.std(scores[0])))\n","    for i in range(len(scores)):\n","        plt.axhline(np.median(scores[i]), color='orange', \n","                    linestyle='--', linewidth=0.5)\n","    plt.boxplot(scores, labels=labels)\n","    plt.ylim(bottom=0.6, top=1.0)\n","    \n","    plt.show()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### LogisticRegression ----\n","\n","my_model = LogisticRegression()\n","cv_scores = cross_val_score(my_model, X, y, scoring='f1', cv=20)\n","print(\"F1: %0.4f (+/- %0.2f)\" % (np.median(cv_scores), np.std(cv_scores)))\n","plot_scores([cv_scores], ['LR'])\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### GradientBoostingClassifier ----\n","\n","from sklearn.datasets import make_classification\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.model_selection import train_test_split\n","\n","my_model = GradientBoostingClassifier(random_state=0)\n","gbc_scores = cross_val_score(my_model, X, y, scoring='f1', cv=10)\n","print(\"F1: %0.4f (+/- %0.2f)\" % (np.median(cv_scores), np.std(cv_scores)))\n","plot_scores([gbc_scores, cv_scores], ['GBC', 'LR'])\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#### Hyperparameter tunning ----\n","\n","# C:\\Users\\juanb\\OneDrive\\GMBD\\STATISTICAL PROGRAMMING - PYTHON (MBD-EN-BL2020J-1_32R203_380389)\\Session13_VC_Sklearn\\SckitLearn-Students\\05 - ScikitLearn.ipynb\n","# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n","# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n","\n","# TODO: Buscar hyperparameters https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","grid = {\n","    'max_depth':range(5,16),\n","    'min_samples_split':range(200,1001),\n","    \"random_state\": [42]\n","}\n","\n","gbc_grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid = grid, cv = 3)\n","gbc_grid_search.fit(X, y)\n","optimal_model = gbc_grid_search.best_estimator_\n","\n","print(\"Fine Tuned Model: {0}\".format(optimal_model))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# TODO: PCA --> LDA/QDA, luego modelo, y veo como queda. Sino, cambiar modelo"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}