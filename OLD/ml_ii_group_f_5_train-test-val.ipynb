{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/956/166/datas/original.png\" width=100 align=\"right\">\n",
    " <div style=\"font-size: large;\"><b>Group Work</b> - Machine Learning II</div><br>\n",
    " <br>\n",
    " <br>\n",
    " <b>Group F</b><br>\n",
    " <br>\n",
    " <a href=\"mailto:alhagbani@student.ie.edu\">Abdulaziz Alhagbani</a><br>\n",
    " <a href=\"mailto:juanbretti@student.ie.edu\">Juan Pedro Bretti Mandarano</a><br>\n",
    " <a href=\"mailto:alexander.madzhirov@student.ie.edu\">Aleksandar Madzhirov</a><br>\n",
    " <a href=\"mailto:esperanza.magpantay@student.ie.edu\">Esperanza Magpantay</a><br>\n",
    " <a href=\"mailto:addison.pelayo@student.ie.edu\">Addison Pelayo</a><br>\n",
    " <br>\n",
    " Delivery: OCT/2020<br>\n",
    " <br>\n",
    " <a href=\"https://www.ie.edu/school-human-sciences-technology/masters/global-master-business-analytics-big-data/\">Global Master in Business Analytics and Big Data</a> | <a href=\"https://www.ie.edu/\">IE</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Loading libraries\n",
    " Loading necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading libraries ----\n",
    "\n",
    "# General usage\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reporting\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Modeling\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, matthews_corrcoef, confusion_matrix, precision_score, classification_report, accuracy_score, multilabel_confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "# Encoders\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import category_encoders as ce\n",
    "\n",
    "# Scaler\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "# SymbolicTransformer\n",
    "from gplearn.genetic import SymbolicTransformer\n",
    "\n",
    "# ReliefF\n",
    "from typing import List\n",
    "from skrebate import ReliefF\n",
    "import random\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "# Model\n",
    "from datetime import datetime\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Save model\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Loading data\n",
    " Loading the `modelling set data` from the CSV file.<br>\n",
    " We are splitting rows between `train` and `test` for this exercise. To evaluate the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading data ----\n",
    "\n",
    "df = pd.read_csv('raw/modeling_set.csv')\n",
    "full_execution = False\n",
    "target = 'round_winner'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split dataset ----\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "\n",
    "# Train\n",
    "df = X\n",
    "df[target] = y\n",
    "\n",
    "# Test\n",
    "df_test = X_test\n",
    "df_test[target] = y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # EDA\n",
    " Exploratory data analysis (EDA) using the library `pandas_profiling`, [link to the repository](https://github.com/pandas-profiling/pandas-profiling).<br>\n",
    " This library creates a very comprehensive HTML file.<br>\n",
    " The file will be provided with this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDA ----\n",
    "pd.set_option('display.max_rows', 100)\n",
    "if full_execution:\n",
    "    df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_execution:\n",
    "    profile = ProfileReport(df, title=\"CS:GO >> Before\", minimal=True)\n",
    "    profile.to_file(\"storage/df_report_before.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Feature construction\n",
    " Using the libraries and methods presented in class, we are constructing features to our source dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## New column names\n",
    " For normalization inside the code, we are assigning variables to some column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New column names ----\n",
    "# OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 3\\Notebooks on feature engineering\\feature importance.ipynb\n",
    "\n",
    "target = \"round_winner\"\n",
    "target_encoded = \"round_winner_encoded\"\n",
    "map_ = 'map'\n",
    "map_encoded = 'map_encoded'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature construction ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Feature construction\n",
    " We created ad-hoc features, based on the strategies:\n",
    " * Compare results form the two teams\n",
    " * Considering that having higher number of players and planted the bomb, defines a winner\n",
    " * Summing all the number of weapons per team\n",
    " * Appling `log` transformation to highly skewed distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manual feature construction ----\n",
    "\n",
    "def feature_construction(df):\n",
    "    \"\"\"Manual feature construction, based on ad-hoc interpretation of the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Source data frame\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same as source\n",
    "    \"\"\"\n",
    "    # Cast `bool` to `float`\n",
    "    df['bomb_planted'] = 1.*df['bomb_planted']\n",
    "\n",
    "    # Operations between comparable columns per team\n",
    "    columns_ct_compare = ['ct_score', 'ct_health', 'ct_armor', 'ct_money', 'ct_players_alive']\n",
    "    columns_t_compare = ['t_score', 't_health', 't_armor', 't_money', 't_players_alive']\n",
    "\n",
    "    for i, j in zip(columns_ct_compare, columns_t_compare):\n",
    "        df[f'compare_diff_{i}'] = df[i]-df[j]\n",
    "        df[f'compare_sum_{i}'] = df[i]+df[j]\n",
    "\n",
    "    columns_compare = []\n",
    "    for col in df.columns:\n",
    "        if col.find('compare_') != -1:\n",
    "            columns_compare.append(col)\n",
    "\n",
    "    # Who is the game winner\n",
    "    df['manual_round_winner'] = 1.*((df['bomb_planted'] == 1.) & (df['t_players_alive'] > df['ct_players_alive']))\n",
    "\n",
    "    # Sum weapons per team\n",
    "    columns_ct_weapon =['ct_weapon_ak47', 'ct_weapon_aug', 'ct_weapon_awp', 'ct_weapon_bizon', 'ct_weapon_cz75auto', 'ct_weapon_elite', 'ct_weapon_famas', 'ct_weapon_g3sg1', 'ct_weapon_galilar', 'ct_weapon_glock', 'ct_weapon_m249', 'ct_weapon_m4a1s', 'ct_weapon_m4a4', 'ct_weapon_mac10', 'ct_weapon_mag7', 'ct_weapon_mp5sd', 'ct_weapon_mp7', 'ct_weapon_mp9', 'ct_weapon_negev', 'ct_weapon_nova', 'ct_weapon_p90', 'ct_weapon_r8revolver', 'ct_weapon_sawedoff', 'ct_weapon_scar20', 'ct_weapon_sg553', 'ct_weapon_ssg08', 'ct_weapon_ump45', 'ct_weapon_xm1014', 'ct_weapon_deagle', 'ct_weapon_fiveseven', 'ct_weapon_usps', 'ct_weapon_p250', 'ct_weapon_p2000', 'ct_weapon_tec9']\n",
    "    columns_ct_grenade =['ct_grenade_hegrenade', 'ct_grenade_flashbang', 'ct_grenade_smokegrenade', 'ct_grenade_incendiarygrenade', 'ct_grenade_molotovgrenade', 'ct_grenade_decoygrenade']\n",
    "    columns_t_weapon = ['t_weapon_ak47', 't_weapon_aug', 't_weapon_awp', 't_weapon_bizon', 't_weapon_cz75auto', 't_weapon_elite', 't_weapon_famas', 't_weapon_g3sg1', 't_weapon_galilar', 't_weapon_glock', 't_weapon_m249', 't_weapon_m4a1s', 't_weapon_m4a4', 't_weapon_mac10', 't_weapon_mag7', 't_weapon_mp5sd', 't_weapon_mp7', 't_weapon_mp9', 't_weapon_negev', 't_weapon_nova', 't_weapon_p90', 't_weapon_r8revolver', 't_weapon_sawedoff', 't_weapon_scar20', 't_weapon_sg553', 't_weapon_ssg08', 't_weapon_ump45', 't_weapon_xm1014', 't_weapon_deagle', 't_weapon_fiveseven', 't_weapon_usps', 't_weapon_p250', 't_weapon_p2000', 't_weapon_tec9']\n",
    "    columns_t_grenade = ['t_grenade_hegrenade', 't_grenade_flashbang', 't_grenade_smokegrenade', 't_grenade_incendiarygrenade', 't_grenade_molotovgrenade', 't_grenade_decoygrenade']\n",
    "\n",
    "    df['columns_ct_weapon'] = df.loc[:, columns_ct_weapon].sum(axis=1)\n",
    "    df['columns_ct_grenade'] = df.loc[:, columns_ct_grenade].sum(axis=1)\n",
    "    df['columns_t_weapon'] = df.loc[:, columns_t_weapon].sum(axis=1)\n",
    "    df['columns_t_grenade'] = df.loc[:, columns_t_grenade].sum(axis=1)\n",
    "\n",
    "    df['columns_ct_weapon_grenade'] = df.loc[:, ['columns_ct_weapon', 'columns_ct_grenade']].sum(axis=1)\n",
    "    df['columns_t_weapon_grenade'] = df.loc[:, ['columns_t_weapon', 'columns_t_grenade']].sum(axis=1)\n",
    "\n",
    "    # Apply log to some columns\n",
    "    # 'ct_score', 't_score', \n",
    "    columns_log = ['time_left', 'ct_health', 't_health', 'ct_armor', 't_armor', 'ct_money', 't_money']\n",
    "    df[columns_log] = df[columns_log].apply(lambda x: np.log(x+1))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = feature_construction(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Remove and filtering\n",
    " Using several of the techniques presented in class, we remove and filter rows and columns.<br>\n",
    " For example,\n",
    " * columns where all the values are NA,\n",
    " * columns with the same value in all the column,\n",
    " * duplicate rows\n",
    " * and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove and filtering ----\n",
    "\n",
    "def remove_and_filtering(df):\n",
    "    \"\"\"Removes and filters rows based on number of NA, constant values, high cardinality, missing values and duplicates.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Source data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same structure DataFrame\n",
    "    \"\"\"\n",
    "    # Remove fully NA columns\n",
    "    print('** Remove fully NA columns')\n",
    "    print(f'Shape before {df.shape}')\n",
    "    df.dropna(axis='columns', how='all', inplace=True)\n",
    "    print(f'Shape after {df.shape}')\n",
    "\n",
    "    # Remove constant value columns\n",
    "    print('** Remove constant value columns')\n",
    "    print(f'Shape before {df.shape}')\n",
    "    col_unique = df.columns[df.nunique()==1]\n",
    "    df.drop(col_unique, axis=1, inplace=True)\n",
    "    print(f'Shape after {df.shape}')\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    print('** Remove duplicate rows')\n",
    "    print(f'Number of duplicates {df.duplicated().sum()} rows to be removed')\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = remove_and_filtering(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## TargetEncoder\n",
    " The following, converts the categorical columns into numerical.<br>\n",
    " It assigns an `integer` to each categorical `string`.<br>\n",
    " We tested using `pd.get_dummies` and also `LabelEncoder`, the second with not significant difference in results. Because we are running this model in a computer with high amount of RAM, we are going to use for the final model `One-Hot Encoding`.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TargetEncoder ----\n",
    "# OneDrive/GMBD/MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)/Session 4 - Feature Engineering/FE BlindCredit example (original 2).ipynb\n",
    "\n",
    "def one_hot_encoder(df, column_):\n",
    "    \"\"\"One hot encoder using Pandas\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Source data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same as source\n",
    "    \"\"\"\n",
    "    dummied = pd.get_dummies(df, drop_first=True, columns=column_)\n",
    "    return dummied\n",
    "\n",
    "def label_encoder(df, col_source, col_target, encoder=None):\n",
    "    \"\"\"Encode columns from string to integers.\n",
    "\n",
    "    Args:\n",
    "        df (DatFrame): Source data\n",
    "        col_source (str): Name of the column to encode\n",
    "        col_target (str): New name of the column to encode\n",
    "        encoder (obj, optional): Object of the type 'LabelEncoder'. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same as source\n",
    "        Encoder: From the type 'LabelEncoder' \n",
    "    \"\"\"\n",
    "    if encoder is None:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(df[col_source])\n",
    "    col = encoder.transform(df[col_source])\n",
    "    df[col_target] = col\n",
    "    df = df.drop(col_source, axis=1)\n",
    "    return df, encoder\n",
    "\n",
    "# df, enc_le_map = label_encoder(df, map_, map_encoded)\n",
    "df = one_hot_encoder(df, [map_])\n",
    "df, enc_le_target = label_encoder(df, target, target_encoded)\n",
    "\n",
    "# For map_\n",
    "def target_encoder(df, target_encoded, encoder=None):\n",
    "    \"\"\"Encode the column 'map' into a float, using the information from 'target_encoded'.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Source data\n",
    "        target_encoded (str): Target column\n",
    "        encoder (obj, optional): Object of the type 'TargetEncoder'. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same as source\n",
    "        Encoder: From the type 'TargetEncoder'\n",
    "    \"\"\"\n",
    "    df_cat = df.loc[:, target_encoded]\n",
    "    if encoder is None:\n",
    "        encoder = ce.target_encoder.TargetEncoder(cols=map_encoded)\n",
    "        encoder.fit(df.drop(target_encoded, axis=1), df[target_encoded])\n",
    "    df = encoder.transform(df.drop(target_encoded, axis=1), df[target_encoded])\n",
    "    df = pd.concat([df.reset_index().drop(columns=\"index\"), df_cat.reset_index().drop(columns=\"index\")], axis=1)\n",
    "    return df, encoder\n",
    "\n",
    "# df, enc_target = target_encoder(df, target_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## StandardScaler\n",
    " Using the method `StandardScaler`, we are converting the numerical values into a continuos values.<br>\n",
    " This converts the `integers` into continuos `floats`.<br>\n",
    " We also tried with `RobustScaler`, with not improvement in the model accuracy. This is why we are leaving for the final model `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### StandardScaler ----\n",
    "# OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 9 - Forum - Dimensionality Reduction\\Notebook on PCA\\PCA solved_v2.ipynb\n",
    "# Set a variable (features) with the names of all the features BUT the target variable.\n",
    "\n",
    "def scaler_transform(df, target_encoded, encoder=None):\n",
    "    \"\"\"Normalize numerical columns.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Data source\n",
    "        target_encoded (str): Target column name\n",
    "        encoder (obj, optional): Object of the type 'StandardScaler'. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same as source\n",
    "        Encoder: From the type 'StandardScaler'\n",
    "    \"\"\"\n",
    "    df_cat = df[target_encoded]\n",
    "    df_columns = df.columns\n",
    "    if encoder is None:\n",
    "        encoder = StandardScaler()\n",
    "        encoder.fit(df.drop(target_encoded, axis=1))\n",
    "    df = encoder.transform(df.drop(target_encoded, axis=1))\n",
    "    df = pd.DataFrame(df, columns = df_columns[:-1])\n",
    "    df = pd.concat([df.reset_index().drop(columns=\"index\"), df_cat.reset_index().drop(columns=\"index\")], axis=1)\n",
    "    columns_ = df.columns\n",
    "    return df, encoder, columns_\n",
    "\n",
    "df, enc_scaler, columns_scaler = scaler_transform(df, target_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Correlation\n",
    " Calculating `correlation` between variables.<br>\n",
    " This `correlation` will help to filter highly correlated columns.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Correlation ----\n",
    "\n",
    "def correlation_plot(df, target_encoded):\n",
    "    \"\"\"Plot a sorted correlation plot.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Source data\n",
    "        target_encoded (str): Target column\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    df_ = df.drop(target_encoded, axis=1).select_dtypes(exclude=['object'])\n",
    "    df_ = scaler.fit_transform(df_)\n",
    "    cov = np.cov(df_, rowvar=False)\n",
    "    order = np.array(hierarchy.dendrogram(hierarchy.ward(cov),no_plot=True)['ivl'], dtype=\"int\")\n",
    "\n",
    "    plt.imshow(cov[order, :][:, order])\n",
    "\n",
    "# https://stackoverflow.com/a/63536382/3780957\n",
    "def correlation_threshold(x: pd.DataFrame, bound: float):\n",
    "    \"\"\"Filters the provided DataFrame based on the threshold defined at 'bound' parameter.\n",
    "\n",
    "    Args:\n",
    "        x (pd.DataFrame): Source DataFrame\n",
    "        bound (float): Threshold to limit the correlation.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame\n",
    "    \"\"\"\n",
    "    xCorr = x.corr()\n",
    "    xFiltered = xCorr[((xCorr >= bound) | (xCorr <= -bound)) & (xCorr !=1.000)]\n",
    "    xFlattened = xFiltered.unstack().sort_values().drop_duplicates()\n",
    "    return xFlattened\n",
    "\n",
    "def correlation_filter(df, threshold=0.99):\n",
    "    \"\"\"Filters data based on the correlation.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Source data\n",
    "        threshold (float, optional): Threshold for the filter. Defaults to 0.99.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same as source\n",
    "    \"\"\"\n",
    "    cor_ = correlation_threshold(df, threshold)\n",
    "    df.drop(cor_.reset_index()['level_1'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/a/25733562/3780957\n",
    "\n",
    "if full_execution:\n",
    "    correlation_plot(df, target_encoded)    \n",
    "print('Correlation filter')\n",
    "print(f'Shape before {df.shape}')\n",
    "df = correlation_filter(df, .90)\n",
    "print(f'Shape after {df.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## SymbolicTransformer\n",
    " This method presented in class, transform and creates features using multiple operations. This will add new features to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SymbolicTransformer ----\n",
    "# OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 4 - Feature Engineering\\FE BlindCredit example (original 2).ipynb\n",
    "\n",
    "def symbolic_transformer(X, y, encoder=None):\n",
    "    \"\"\"Transform features using multiple operations. This will add new features to the data frame.\n",
    "\n",
    "    Args:\n",
    "        X (DataFrame): Independent features\n",
    "        y (Series): Dependen feature or target\n",
    "        encoder (obj, optional): Object of the type 'SymbolicTransformer'. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Additional columns calculated by the algorithm\n",
    "    \"\"\"\n",
    "    if encoder is None:\n",
    "        function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log',\n",
    "                        'abs', 'neg', 'inv', 'max', 'min']\n",
    "        encoder = SymbolicTransformer(generations=10,\n",
    "                                population_size=1000,\n",
    "                                hall_of_fame=100,\n",
    "                                n_components=12,\n",
    "                                function_set=function_set,\n",
    "                                parsimony_coefficient=0.0005,\n",
    "                                max_samples=0.9,\n",
    "                                verbose=1,\n",
    "                                random_state=123,\n",
    "                                n_jobs=-1)\n",
    "        encoder.fit(X, y)\n",
    "    gp_features = encoder.transform(X)\n",
    "\n",
    "    return gp_features, encoder\n",
    "\n",
    "def symbolic_transformer_fit(df, encoder=None):\n",
    "    \"\"\"Application of the 'SymbolicTransformer' in our data model\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Source data\n",
    "        encoder (obj, optional): Object from the type 'SymbolicTransformer'. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same as source\n",
    "        encoder: Object from the type 'SymbolicTransformer'\n",
    "        columns: list of additional columns created\n",
    "    \"\"\"\n",
    "    gp_features, enc_gp = symbolic_transformer(df.drop(target_encoded, axis=1), df[target_encoded], encoder)\n",
    "\n",
    "    columns_ = df.columns\n",
    "\n",
    "    df_new_features = pd.DataFrame(gp_features)\n",
    "    df_new_features.columns = ['gp{}'.format(i) for i in range(len(list(df_new_features)))]\n",
    "\n",
    "    df = pd.concat([df.reset_index().drop(columns=\"index\"), df_new_features], axis=1)\n",
    "\n",
    "    return df, enc_gp, columns_\n",
    "\n",
    "df, enc_gp, columns_symbolic = symbolic_transformer_fit(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Feature importance\n",
    " In the following section we will select the most relevant features. These will be source for our ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature importance ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Outliers\n",
    " In the following section, we filter `outliers` datapoint.<br>\n",
    " Particularly, we tried the algorithm `IsolationForest`. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Outliers ----\n",
    "\n",
    "def outliers_isolation_forest(df, target_encoded, encoder=None, contamination=0.001):\n",
    "    \"\"\"Using 'Isolation Forest', filters the outliers data points\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Source data\n",
    "        target_encoded (str): Target column name\n",
    "        encoder (obj, optional): Object of the type 'IsolationForest'. Defaults to None.\n",
    "        contamination (float, optional): Threshold to remove the outliers. Defaults to 0.001.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same as source\n",
    "    \"\"\"\n",
    "    if encoder is None:\n",
    "        encoder = IsolationForest(contamination=contamination)\n",
    "        y_pred = encoder.fit(df.drop([target_encoded], axis=1))\n",
    "    y_pred = encoder.predict(df.drop([target_encoded], axis=1))\n",
    "    mask = y_pred != -1\n",
    "    df = df.loc[list(mask), :]\n",
    "    return df, encoder\n",
    "\n",
    "print(f'Shape before {df.shape}')\n",
    "df, enc_outliers = outliers_isolation_forest(df, target_encoded)\n",
    "print(f'Shape after {df.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## RelieF Algorithm [not in use]\n",
    " We try to use an implementation RELIEF algorithm that we can find [here](https://github.com/EpistasisLab/scikit-rebate). We will simply specify how many neighbors to consider when comparing each feature with the rest, to measure differences, and how many features do we want at the end of the process.<br>\n",
    " The only caveat is the algorithm expects the values as Numpy arrays with shapes $(m, p)$ for the features ($m$ is the number of tuples/samples and $p$ is the number of predictors/features), and $(m, 1)$ for the target variable (a 1D numpy array, for which we must use the function `ravel()` from Numpy).\n",
    " After several tries, we decided to not include this algorithm in the final pipeline, because the elimination of columns is not improving the `accuracy` of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ReliefF ranking [not in use] ----\n",
    "# OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 3\\Notebooks on feature engineering\\feature importance.ipynb\n",
    "# Feature importance\n",
    "# Filters\n",
    "\n",
    "def plot_importance(features: List[str], importances: List[float]):\n",
    "    \"\"\"Plot the variable importance of the data frame\n",
    "\n",
    "    Args:\n",
    "        features (List[str]): Features to be included in the plot\n",
    "        importances (List[float]): Float of the value importance per feature\n",
    "    \"\"\"\n",
    "\n",
    "    num_features = len(features)\n",
    "    indices = np.argsort(importances)\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.barh(range(num_features), importances[indices],\n",
    "             color=\"r\",\n",
    "             xerr=np.std(importances),\n",
    "             align=\"center\")\n",
    "    plt.yticks(range(num_features), features[indices])\n",
    "    plt.ylim([-1, num_features])\n",
    "    plt.show()\n",
    "\n",
    "def importance_relieff(X, y, n_features_to_select, n_neighbors, sample_rows, encoder=None, plot=True):\n",
    "    \"\"\"Utilization of the algorithm ReliefF in our dataframe\n",
    "\n",
    "    Args:\n",
    "        X (DataFrame): Independent variables\n",
    "        y (Series): Dependen variable or target\n",
    "        n_features_to_select (int): Number of features to be in the resulting DataFrame\n",
    "        n_neighbors (int): Number of neighbors to be condered for the model\n",
    "        sample_rows (int): Number of sample rows\n",
    "        encoder (obj, optional): Object from the type 'ReliefF'. Defaults to None.\n",
    "        plot (bool, optional): Controls to show or not the 'plot_importance'. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same as source\n",
    "    \"\"\"\n",
    "\n",
    "    sample = random.sample(list(X.index), sample_rows)\n",
    "    sample_features = X.iloc[sample, :].to_numpy()\n",
    "    sample_labels = y.iloc[sample].to_numpy()\n",
    "\n",
    "    if encoder is None:\n",
    "        encoder = ReliefF(n_features_to_select=n_features_to_select, n_neighbors=n_neighbors)\n",
    "        encoder.fit(sample_features, sample_labels)\n",
    "    my_important_features = encoder.transform(sample_features)\n",
    "\n",
    "    print(\"No. of tuples, No. of Columns before ReliefF : \"+str(sample_features.shape)+\n",
    "        \"\\nNo. of tuples, No. of Columns after ReliefF : \"+str(my_important_features.shape))\n",
    "\n",
    "    # Plot the importances, taken from the `encoder` variable.\n",
    "    if plot:\n",
    "        plot_importance(X.columns, abs(encoder.feature_importances_))\n",
    "\n",
    "    # Get the most important column names\n",
    "    my_important_features_names  = [X.columns[i] for i in abs(encoder.top_features_)]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    X = pd.DataFrame(X, columns=my_important_features_names[:my_important_features.shape[1]])\n",
    "\n",
    "    return X, encoder\n",
    "\n",
    "\n",
    "def importance_relieff_fit(df, target_encoded, encoder=None, plot=True):\n",
    "    \"\"\"Implementation of the algorithm 'ReliefF'.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Source data\n",
    "        target_encoded (str): Target column name\n",
    "        encoder (obj, optional): Object of the type 'ReliefF'. Defaults to None.\n",
    "        plot (bool, optional): Controls to show or not the 'plot_importance'. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same as source\n",
    "        encoder: Object of the type 'ReliefF'\n",
    "        columns: List of columns in the resulting data frame \n",
    "    \"\"\"\n",
    "    df = df.reset_index().drop(columns=\"index\")\n",
    "    X = df.drop(target_encoded, axis=1)\n",
    "    y = df[target_encoded]\n",
    "    X_transformed, encoder = importance_relieff(X, y, n_features_to_select=50, n_neighbors=10, sample_rows=10000, encoder=encoder, plot=plot)\n",
    "    df = pd.concat([X_transformed.reset_index().drop(columns=\"index\"), y.reset_index().drop(columns=\"index\")], axis=1)\n",
    "    columns_ = df.columns\n",
    "    return df, encoder, columns_\n",
    "\n",
    "# This filter is not being used. Because the 'accuracy' is better without this filter.\n",
    "# df2, enc_relieff, columns_relieff = importance_relieff_fit(df, target_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PCA\n",
    " The following is an implementation of the `Principal Component Analysis`.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA ----\n",
    "# OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 9 - Forum - Dimensionality Reduction\\Notebook on PCA\\PCA_v2.ipynb\n",
    "\n",
    "name = \"Accent\"\n",
    "cmap = get_cmap(name)  # type: matplotlib.colors.ListedColormap\n",
    "colors = cmap.colors  # type: list\n",
    "\n",
    "def pca_transform(data, target, n=2, encoder=None):\n",
    "    \"\"\"Applies the principal component algorithm to the data frame.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Source data\n",
    "        target (str): Column name\n",
    "        n (int, optional): Number of components to be calculated. Defaults to 2.\n",
    "        encoder (obj, optional): Object of the type 'PCA'. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same as source\n",
    "        List: Variance explained by each PC.\n",
    "        Encoder: Object of the type 'PCA'\n",
    "    \"\"\"\n",
    "    if encoder is None:\n",
    "        encoder = PCA(n_components=n)\n",
    "        encoder = encoder.fit(data.drop([target], axis=1))\n",
    "    principalComponents = encoder.transform(data.drop([target], axis=1))\n",
    "    explained_pca = encoder.explained_variance_ratio_\n",
    "\n",
    "    data_pca1 = pd.DataFrame(data=principalComponents).reset_index().drop(columns=\"index\")\n",
    "    data_pca2 = data[target].reset_index().drop(columns=\"index\")\n",
    "    data_pca = pd.concat([data_pca1, data_pca2], axis=1)\n",
    "\n",
    "    # Setting columns name\n",
    "    columns = [f\"PC{s}\" for s in range(1, n + 1)]\n",
    "    columns.append(target)\n",
    "    data_pca.columns = columns\n",
    "\n",
    "    return data_pca, explained_pca, encoder\n",
    "\n",
    "# https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "def pca_plot_scatter(data, target, axis1=1, axis2=2):\n",
    "    \"\"\"Plots the PCA using a scatter plot.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Source data\n",
    "        target (str): Target column name\n",
    "        axis1 (int, optional): Number of the PC to be plotted. Defaults to 1.\n",
    "        axis2 (int, optional): Number of the PC to be plotted. Defaults to 2.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_xlabel(f\"Principal Component {axis1}\", fontsize=15)\n",
    "    ax.set_ylabel(f\"Principal Component {axis2}\", fontsize=15)\n",
    "    ax.set_title(\"Component PCA\", fontsize=20)\n",
    "    targets = data[target].unique()\n",
    "    for target_, color in zip(targets, colors):\n",
    "        indicesToKeep = data[target] == target_\n",
    "        ax.scatter(\n",
    "            data.loc[indicesToKeep, f\"PC{axis1}\"],\n",
    "            data.loc[indicesToKeep, f\"PC{axis2}\"],\n",
    "            color=color,\n",
    "            s=50,\n",
    "        )\n",
    "    ax.legend(targets)\n",
    "    ax.grid()\n",
    "\n",
    "def pca_plot_density(data, target):\n",
    "    \"\"\"Plots the PCA using densities\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Source data\n",
    "        target (str): Target column name\n",
    "    \"\"\"\n",
    "    # categories = data[target].unique()\n",
    "    category_series = data[target]\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3)\n",
    "    fig.set_size_inches(14, 10)\n",
    "\n",
    "    feature_names = list(set(data.columns) - set([target]))\n",
    "\n",
    "    for subplot, feature in enumerate(feature_names):\n",
    "        x, y = int(subplot / 3), subplot % 3\n",
    "        for value in data[target].unique():\n",
    "            sns.distplot(\n",
    "                data[feature][category_series == value],\n",
    "                hist=False,\n",
    "                kde=True,\n",
    "                kde_kws={\"shade\": True},\n",
    "                label=str(value),\n",
    "                ax=axs[x, y],\n",
    "            )\n",
    "        axs[x, y].set_title(feature)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_explained_variance, enc_pca = pca_transform(data=df, target=target_encoded, n=19)\n",
    "if full_execution:\n",
    "    print(np.cumsum(df_explained_variance).round(4))\n",
    "    pca_plot_scatter(data=df, target=target_encoded, axis1=1, axis2=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # EDA after preprocessing\n",
    " This step is for internal control of the **preprocessing** steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDA after preprocessing ----\n",
    "if full_execution:\n",
    "    profile = ProfileReport(df, title=\"CS:GO >> After\", minimal=True)\n",
    "    profile.to_file(\"storage/df_report_after.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training model ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training model\n",
    " We have no mayor problems to create the training dataset, because we have `balanced` target.<br>\n",
    " Follwing, different **machine learning** models will be tested. We will select the one that provides us the highest `accuracy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions\n",
    " The following functions were included as helpers for the training, plot and timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Auxiliary functions ----\n",
    "# OneDrive\\GMBD\\MACHINE LEARNING II (MBD-EN-BL2020J-1_32R202_380379)\\Session 5 - EvaluationMetrics\\Evaluation and Validation.ipynb\n",
    "\n",
    "def plot_scores(scores, labels):\n",
    "    \"\"\"\n",
    "    Receives scores (one or several arrays) and plots a scatter to the left with\n",
    "    the values of the first one, and a boxplot with all of them to the right.\n",
    "    \n",
    "    Arguments\n",
    "        scores: single list of scores, or list of lists of scores.\n",
    "        labels: single label or list of labels identifying the scores passed\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Scores from {}.'.format(labels[0]))\n",
    "    plt.scatter(range(len(scores[0])), scores[0])\n",
    "    plt.axhline(np.median(scores[0]), color='orange', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('{} scores stdev={:.4f}'.format(labels[0], np.std(scores[0])))\n",
    "    for i in range(len(scores)):\n",
    "        plt.axhline(np.median(scores[i]), color='orange', \n",
    "                    linestyle='--', linewidth=0.5)\n",
    "    plt.boxplot(scores, labels=labels)\n",
    "    plt.ylim(bottom=0.6, top=1.0)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def timer(start_time=None):\n",
    "    \"\"\"Create a 'timer' object to measure execution time \n",
    "\n",
    "    Args:\n",
    "        start_time (datetime[64], optional): End time when set. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        str: Time elapsed since execution\n",
    "    \"\"\"\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "X = df.drop([target_encoded], axis=1)\n",
    "y = df[target_encoded]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## LogisticRegression\n",
    " The first model we are going to train is `LogisticRegression`.<br>\n",
    " It does not have any relevant tunning parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LogisticRegression ----\n",
    "lr_model = LogisticRegression()\n",
    "lr_scores = cross_val_score(lr_model, X, y, scoring='accuracy', cv=20, n_jobs=-1)\n",
    "print(\"Accuracy: %0.4f (+/- %0.2f)\" % (np.median(lr_scores), np.std(lr_scores)))\n",
    "plot_scores([lr_scores], ['LR'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## RandomForestClassifier\n",
    " We also trained the `RandomForestClassifier` using the default parameters. This is a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RandomForestClassifier ----\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "rf_scores = cross_val_score(rf_model, X, y, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy: %0.4f (+/- %0.2f)\" % (np.median(rf_scores), np.std(rf_scores)))\n",
    "plot_scores([rf_scores, lr_scores], ['RF', 'LR'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Hyperparameter tunning `RandomForestClassifier`, grid search\n",
    " Having such impressive results from this algorithm, we are tunning the hyperparameters. In particular, we are using `GridSearchCV` with a `cross validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hyperparameter tunning RandomForestClassifier, grid search ----\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [200, 600],\n",
    "    'max_depth': [30, 100],\n",
    "    'min_samples_split': [2, 10],\n",
    "    'min_samples_leaf': [1, 2, 5] \n",
    "    }\n",
    "cv_ = 3\n",
    "\n",
    "rf_model_grid = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "rf_model_search = GridSearchCV(rf_model_grid, param_grid=params, scoring='accuracy', n_jobs=-1, cv=cv_, verbose=3)\n",
    "\n",
    "# Here we go\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "rf_model_search.fit(X, y)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Checking the accuracy of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the accuracy of the best model\n",
    "\n",
    "rf_model_after_search = rf_model_search.best_estimator_\n",
    "rf_model_after_search_scores = cross_val_score(rf_model_after_search, X, y, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "print(\"Accuracy: %0.4f (+/- %0.2f)\" % (np.median(rf_model_after_search_scores), np.std(rf_model_after_search_scores)))\n",
    "plot_scores([rf_model_after_search_scores, rf_scores, lr_scores], ['RF tunned', 'RF', 'LR'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Hyperparameter tunning RandomForestClassifier, bayesian search\n",
    " Five different parameters will be tunned using random search.<br>\n",
    " The performance of this algorithm hasn't improved. For speeding up this *notebook*, we had reduced the number of *parameter combination*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hyperparameter tunning RandomForestClassifier, bayesian search ----\n",
    "# https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn\n",
    "# https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html\n",
    "# https://neptune.ai/blog/scikit-optimize\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [200, 600, 1200],\n",
    "    'max_depth': [30, 100, 200],\n",
    "    'min_samples_split': [2, 10],\n",
    "    'min_samples_leaf': [1, 2, 5] \n",
    "    }\n",
    "\n",
    "folds = 3\n",
    "param_comb = 100\n",
    "cv_ = 3\n",
    "\n",
    "rf_model_bayes = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "rf_model_bayes_search = BayesSearchCV(rf_model_bayes, search_spaces=params, n_iter=param_comb, scoring='accuracy', n_jobs=-1, cv=cv_, verbose=3, random_state=42)\n",
    "\n",
    "# Here we go\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "rf_model_bayes_search.fit(X, y)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/45074887/3780957\n",
    "# Checking the accuracy of the best model\n",
    "\n",
    "rf_model_after_bayes_search = rf_model_bayes_search.best_estimator_\n",
    "rf_score_after_bayes = cross_val_score(rf_model_after_bayes_search, X, y, scoring='accuracy', cv=10)\n",
    "print(\"Accuracy: %0.4f (+/- %0.2f)\" % (np.median(rf_score_after_bayes), np.std(rf_score_after_bayes)))\n",
    "# plot_scores([xgb_scores_tunned, lr_scores], \\\n",
    "#    ['XGB tunned', 'LR'])\n",
    "plot_scores([rf_score_after_bayes, xgb_scores, knn_scores, rf_model_after_search_scores, rf_scores, lr_scores], \\\n",
    "    ['XGB tunned', 'KNN', 'RF tunned', 'RF', 'LR'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## KNeighborsClassifier\n",
    " We tried `KNeighborsClassifier`. Classifier implementing the k-nearest neighbors vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KNeighborsClassifier ----\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10, n_jobs=-1)\n",
    "knn_scores = cross_val_score(knn_model, X, y, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "print(\"Accuracy: %0.4f (+/- %0.2f)\" % (np.median(knn_scores), np.std(knn_scores)))\n",
    "plot_scores([knn_scores, rf_model_after_search_scores, rf_scores, lr_scores], ['KNN', 'RF tunned', 'RF', 'LR'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## XGBoost\n",
    " XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. <br>\n",
    " We had set the library to run in `GPU` to increase tunning performance.<br>\n",
    " The performance is very good. We will try hyperparameter tunning.<br>\n",
    " More about XGBoost at the [link](https://xgboost.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XGBoost ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Hyperparameter tunning XGBoost, bayesian search\n",
    " Five different parameters will be tunned using random search.<br>\n",
    " The performance of this algorithm hasn't improved. For speeding up this *notebook*, we had reduced the number of *parameter combination*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hyperparameter tunning XGBoost, bayesian search ----\n",
    "# https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn\n",
    "# https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html\n",
    "# https://neptune.ai/blog/scikit-optimize\n",
    "\n",
    "params = {\n",
    "        'learning_rate': [0.01, 0.3, 0.5],\n",
    "        'min_child_weight': [None, 0, 1, 5, 10],\n",
    "        'gamma': [None, 0, 0.5, 1, 1.5, 2, 5],\n",
    "        'colsample_bytree': [None, 0.6, 0.8, 1.0],\n",
    "        'max_depth': [10],\n",
    "        'subsample': [0.75, 1],\n",
    "        'n_estimators': [100, 500],\n",
    "        'max_delta_step': [0.0],\n",
    "        'colsample_bylevel': [1.0],\n",
    "        'reg_alpha': [0.0],\n",
    "        'reg_lambda': [1.0],\n",
    "        'base_score': [0.5],\n",
    "        'missing': [None]\n",
    "        }\n",
    "\n",
    "folds = 3\n",
    "param_comb = 100\n",
    "cv_ = 3\n",
    "\n",
    "xgb_model_bayes = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, tree_method='gpu_hist', gpu_id=0, nthread=-1)\n",
    "xgb_model_search_bayes = BayesSearchCV(xgb_model_bayes, search_spaces=params, n_iter=param_comb, scoring='accuracy', n_jobs=-1, cv=cv_, verbose=3, random_state=42)\n",
    "\n",
    "# Here we go\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "xgb_model_search_bayes.fit(X, y)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/45074887/3780957\n",
    "# Checking the accuracy of the best model\n",
    "\n",
    "xgb_model_after_bayes_search = xgb_model_search_bayes.best_estimator_\n",
    "xgb_scores_bayes_tunned = cross_val_score(xgb_model_after_bayes_search, X, y, scoring='accuracy', cv=10)\n",
    "print(\"Accuracy: %0.4f (+/- %0.2f)\" % (np.median(xgb_scores_bayes_tunned), np.std(xgb_scores_bayes_tunned)))\n",
    "# plot_scores([xgb_scores_tunned, lr_scores], \\\n",
    "#    ['XGB tunned', 'LR'])\n",
    "plot_scores([xgb_scores_bayes_tunned, xgb_scores, knn_scores, rf_model_after_search_scores, rf_scores, lr_scores], \\\n",
    "    ['XGB tunned', 'KNN', 'RF tunned', 'RF', 'LR'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Testing dataset\n",
    " As mentioned before, the *testing* dataset has been removed to improve the performance for the final assignment validation `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing dataset ----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_accuracy(model, df=df_test, decimals=2):\n",
    "    \"\"\"Print model `accuracy`\n",
    "\n",
    "    Args:\n",
    "        model (object): Sklearn model\n",
    "        y_pred (Series, optional): Dependent variable. Defaults to y_pred.\n",
    "        decimals (int, optional): Number of decimals to print the `accuracy`. Defaults to 2.\n",
    "    \"\"\"\n",
    "    X_validation = df.drop([target_encoded], axis=1)\n",
    "    y_validation = df[target_encoded]\n",
    "    y_pred = model.predict(X_validation)\n",
    "    print(model)\n",
    "    print(f'Accuracy: {(accuracy_score(y_validation, y_pred)*100).round(decimals)}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Preprocessing\n",
    " Applied the *preprocessing* pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing ----\n",
    "\n",
    "# Manual feature construction\n",
    "df_test = feature_construction(df_test)\n",
    "# TargetEncoder\n",
    "df_test = one_hot_encoder(df_test, [map_])\n",
    "df_test, _ = label_encoder(df_test, target, target_encoded, enc_le_target)\n",
    "# StandardScaler\n",
    "df_test, _, _ = scaler_transform(df_test.loc[:, columns_scaler], target_encoded, enc_scaler)\n",
    "# SymbolicTransformer\n",
    "df_test, _, _ = symbolic_transformer_fit(df_test.loc[:, columns_symbolic], enc_gp)\n",
    "# PCA\n",
    "df_test, _, _ = pca_transform(data=df_test, target=target_encoded, n=19, encoder=enc_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model predict\n",
    " Applied the *tunned* models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model predict ----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy(lr_model.fit(X, y), df=df_test)\n",
    "model_accuracy(xgb_model_after_bayes_search, df=df_test)\n",
    "model_accuracy(rf_model.fit(X, y), df=df_test)\n",
    "model_accuracy(rf_model_after_search, df=df_test)\n",
    "model_accuracy(knn_model.fit(X, y), df=df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Storing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model save ----\n",
    "# https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/\n",
    "\n",
    "# filename = 'storage/lr_model.sav'\n",
    "# pickle.dump(lr_model, open(filename, 'wb'))\n",
    "\n",
    "filename = 'storage/xgb_model_after_bayes_search.sav'\n",
    "pickle.dump(xgb_model_after_bayes_search, open(filename, 'wb'))\n",
    "\n",
    "# filename = 'storage/rf_model.sav'\n",
    "# pickle.dump(rf_model, open(filename, 'wb'))\n",
    "\n",
    "filename = 'storage/rf_model_after_search.sav'\n",
    "pickle.dump(rf_model_after_search, open(filename, 'wb'))\n",
    "\n",
    "# filename = 'storage/knn_model.sav'\n",
    "# pickle.dump(knn_model, open(filename, 'wb'))\n",
    "\n",
    "# filename = 'storage/knn_model_after_search.sav'\n",
    "# pickle.dump(knn_model_after_search, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation dataset\n",
    " The class validation will be done using the following dataset `modeling_set.csv`.<br>\n",
    " Here, we also applied the *preprocessing* and the different models to test the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation dataset ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Loading data\n",
    " Loading the previously mentioned `validation_set.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading data ----\n",
    "df_validation = pd.read_csv('raw/validation_set.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Preprocessing\n",
    " Applied the *preprocessing* pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing ----\n",
    "\n",
    "# Manual feature construction\n",
    "df_validation = feature_construction(df_validation)\n",
    "# TargetEncoder\n",
    "df_validation = one_hot_encoder(df_validation, [map_])\n",
    "df_validation, _ = label_encoder(df_validation, target, target_encoded, enc_le_target)\n",
    "# StandardScaler\n",
    "df_validation, _, _ = scaler_transform(df_validation.loc[:, columns_scaler], target_encoded, enc_scaler)\n",
    "# SymbolicTransformer\n",
    "df_validation, _, _ = symbolic_transformer_fit(df_validation.loc[:, columns_symbolic], enc_gp)\n",
    "# PCA\n",
    "df_validation, _, _ = pca_transform(data=df_validation, target=target_encoded, n=19, encoder=enc_pca)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model predict\n",
    " Applied the *tunned* models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model predict ----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy(lr_model.fit(X, y), df=df_validation)\n",
    "model_accuracy(xgb_model_after_bayes_search, df=df_validation)\n",
    "model_accuracy(rf_model.fit(X, y), df=df_validation)\n",
    "model_accuracy(rf_model_after_search, df=df_validation)\n",
    "model_accuracy(rf_model_after_bayes_search, df=df_validation)\n",
    "model_accuracy(knn_model.fit(X, y), df=df_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Prediction\n",
    " Prediction using our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction ----\n",
    "X_validation = df.drop([target_encoded], axis=1)\n",
    "prediction_ = rf_model_after_search.predict(X_validation)\n",
    "prediction_ = enc_le_target.inverse_transform(prediction_)\n",
    "\n",
    "np.savetxt(\"results/prediction.csv\", prediction_, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
